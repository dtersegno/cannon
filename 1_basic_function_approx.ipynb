{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42323c19",
   "metadata": {},
   "source": [
    "function approximation with keras\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a0a8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1daced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feed_forward_continuous(keras.models.Sequential):\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_size = 128):\n",
    "        super().__init__()\n",
    "        self.add(tf.keras.layers.Dense(num_inputs, activation = 'relu', kernel_initializer='he_normal'))\n",
    "        for layer in range(num_hidden):\n",
    "            self.add(tf.keras.layers.Dense(hidden_size, activation = 'relu', kernel_initializer='he_normal'))\n",
    "        self.add(tf.keras.layers.Dense(num_outputs, activation = 'sigmoid'))\n",
    "        self.compile(optimizer = 'adam', loss = 'mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ead99e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_model = feed_forward_continuous(1,2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb8c484",
   "metadata": {},
   "source": [
    "# Approximate Sine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ccd072",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1000\n",
      "Epoch 2000\n",
      "Epoch 3000\n",
      "Epoch 4000\n",
      "Epoch 5000\n",
      "Epoch 6000\n",
      "Epoch 7000\n"
     ]
    }
   ],
   "source": [
    "#make rng\n",
    "seed=2023\n",
    "rng = np.random.default_rng(seed)\n",
    "pi = np.pi\n",
    "input_width = 2*pi\n",
    "input_center = pi\n",
    "\n",
    "epoch_losses = 0.\n",
    "num_epochs = 10000\n",
    "batch_size = 128\n",
    "\n",
    "sin_model = feed_forward_continuous(1, 2, 1, 128)\n",
    "\n",
    "#training loop\n",
    "approx_this_function = tf.math.sin\n",
    "loss_fn = tf.losses.mean_squared_error\n",
    "optimizer = tf.optimizers.Adam(learning_rate=.1)\n",
    "history = []\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.learning_rate = optimizer.learning_rate*0.999\n",
    "    if epoch%1000==0:\n",
    "        print(f'Epoch {epoch}')\n",
    "    batch_Xes = []\n",
    "    batch_preds = []\n",
    "    batch_y = []\n",
    "    losses = []\n",
    "    with tf.GradientTape() as tape:\n",
    "        for batch_guy in range(batch_size):\n",
    "            x = input_width*(rng.random()-0.5) + input_center\n",
    "            batch_Xes.append(x)\n",
    "            pred = sin_model.call(tf.convert_to_tensor([[x]]))\n",
    "            y = approx_this_function(x)\n",
    "            batch_preds.append(pred)\n",
    "            batch_y.append(y)\n",
    "            losses.append(loss_fn(pred,y))\n",
    "        loss = tf.reduce_mean(losses)\n",
    "        grads = tape.gradient(loss, sin_model.trainable_variables)\n",
    "        #if epoch%50 == 0:\n",
    "        #    print(sin_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, sin_model.trainable_variables))\n",
    "        #if epoch%50 == 0:\n",
    "        #    print(sin_model.trainable_variables)\n",
    "    history.append([batch_Xes, batch_preds, batch_y])\n",
    "    loss_history.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (12,6)\n",
    "plt.figure(figsize = figsize)\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fdab2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(batch_Xes, batch_y)\n",
    "plt.xlim(0,2*pi)\n",
    "plt.show()\n",
    "y = sin_model.predict(np.linspace(0, 2*pi, 50))\n",
    "plt.xlim(0,2*pi)\n",
    "plt.plot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887eb3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "38ace508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.040932547>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "optimizer.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "a4ca2e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "No gradient defined for operation'IteratorGetNext' (op type: IteratorGetNext). In general every operation must have an associated `@tf.RegisterGradient` for correct autodiff, which this op is lacking. If you want to pretend this operation is a constant in your program, you may insert `tf.stop_gradient`. This can be useful to silence the error in cases where you know gradients are not needed, e.g. the forward pass of tf.custom_gradient. Please see more details in https://www.tensorflow.org/api_docs/python/tf/custom_gradient.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9820/521280302.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_width\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_center\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msin_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mapprox_this_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[0;32m    621\u001b[0m               \u001b[0mgrad_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_call\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_grad_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m               raise LookupError(\n\u001b[0m\u001b[0;32m    624\u001b[0m                   \u001b[1;34m\"No gradient defined for operation\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m                   \u001b[1;34mf\"'{op.name}' (op type: {op.type}). \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: No gradient defined for operation'IteratorGetNext' (op type: IteratorGetNext). In general every operation must have an associated `@tf.RegisterGradient` for correct autodiff, which this op is lacking. If you want to pretend this operation is a constant in your program, you may insert `tf.stop_gradient`. This can be useful to silence the error in cases where you know gradients are not needed, e.g. the forward pass of tf.custom_gradient. Please see more details in https://www.tensorflow.org/api_docs/python/tf/custom_gradient."
     ]
    }
   ],
   "source": [
    "#make rng\n",
    "seed=2022\n",
    "rng = np.random.default_rng(seed)\n",
    "pi = np.pi\n",
    "input_width = 4*pi\n",
    "input_center = 2*pi\n",
    "\n",
    "epoch_losses = 0.\n",
    "num_epochs = 640\n",
    "batch_size = 256\n",
    "\n",
    "sin_model = feed_forward_continuous(1, 1, 1, 128)\n",
    "\n",
    "#training loop\n",
    "approx_this_function = tf.math.sin\n",
    "loss_fn = tf.losses.mean_squared_error\n",
    "optimizer = tf.optimizers.Adam(learning_rate=.05)\n",
    "history = []\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.learning_rate = optimizer.learning_rate*0.9\n",
    "    if epoch%100==0:\n",
    "        print(f'Epoch {epoch}')\n",
    "    batch_Xes = []\n",
    "    batch_preds = []\n",
    "    batch_y = []\n",
    "    losses = []\n",
    "    with tf.GradientTape() as tape:\n",
    "        x = input_width*(rng.random(batch_size)-0.5) + input_center\n",
    "        pred = sin_model.predict(tf.convert_to_tensor([x]))\n",
    "        y = [ approx_this_function(value) for value in x ]\n",
    "        losses.append(loss_fn(pred,y))\n",
    "        loss = tf.reduce_mean(losses)\n",
    "        grads = tape.gradient(loss, sin_model.trainable_variables)\n",
    "        #if epoch%50 == 0:\n",
    "        #    print(sin_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, sin_model.trainable_variables))\n",
    "        #if epoch%50 == 0:\n",
    "        #    print(sin_model.trainable_variables)\n",
    "    history.append([batch_Xes, batch_preds, batch_y])\n",
    "    loss_history.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15130d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
