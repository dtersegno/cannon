{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade50ac1",
   "metadata": {},
   "source": [
    "Actor-critic cartesian rocket\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4080776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "c7eb234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cartesian_rocket:\n",
    "    def __init__(self,\n",
    "                 dry_mass = 10.,\n",
    "                 wet_mass = 60.,\n",
    "                 isp = 500.,\n",
    "                 time_step = 0.1,\n",
    "                 drag_coef = 0.,\n",
    "                 max_fuel_rate = 1.5, # per second\n",
    "#                  throttle_rate = 0.1 #increase of fuel use \n",
    "                 reference_height = 200, #m, for barometric pressure\n",
    "                 g = 9.81\n",
    "                ):\n",
    "        self.dry_mass = dry_mass\n",
    "        self.wet_mass = wet_mass\n",
    "        self.isp = isp\n",
    "        self.time_step = time_step\n",
    "        self.drag_coef = drag_coef\n",
    "        self.max_fuel_rate = max_fuel_rate\n",
    "#         self.throttle_rate = throttle_rate\n",
    "        \n",
    "        #keep track of time\n",
    "        self.stopwatch = 0.\n",
    "        \n",
    "        #keep track of velocity\n",
    "        self.velocity = np.array([0.,0.])\n",
    "        \n",
    "        #keep track of position\n",
    "        self.position = np.array([0.,0.])\n",
    "        \n",
    "        #drag\n",
    "        self.area = 1.\n",
    "        self.reference_height = reference_height\n",
    "        \n",
    "        #gravity\n",
    "        self.g = np.array([0., -g])\n",
    "        \n",
    "        #history.\n",
    "        self.history_position = [\n",
    "            self.position\n",
    "        ]\n",
    "        self.history_velocity = [\n",
    "            self.velocity\n",
    "        ]\n",
    "        \n",
    "        right = np.array([1.,0.])\n",
    "        up = np.array([0.,1.])\n",
    "        self.directions = {\n",
    "            1:right,\n",
    "            2:up,\n",
    "            3:-right,\n",
    "            4:-up\n",
    "        }\n",
    "    \n",
    "    #increase fuel in direction. Adds to velocity at the cost of fuel\n",
    "    def fire(self, direction_code):\n",
    "        direction = self.directions[direction_code]\n",
    "        fuel_used = self.max_fuel_rate * self.time_step\n",
    "        if fuel_used > self.wet_mass:\n",
    "            fuel_used = self.wet_mass\n",
    "        self.wet_mass -= fuel_used\n",
    "        force = direction*self.time_step*self.isp*np.linalg.norm(self.g)*fuel_used\n",
    "        accel = force / (self.wet_mass + self.dry_mass)\n",
    "        self.velocity += accel\n",
    "        \n",
    "    def apply_drag(self):\n",
    "        atm = np.exp(-self.position[1]/self.reference_height)\n",
    "        drag_force = -0.5*atm*self.drag_coef*self.area*np.linalg.norm(self.velocity)*self.velocity\n",
    "        drag_dv =  self.time_step*drag_force / (self.wet_mass + self.dry_mass)\n",
    "        drag_dv_norm = np.linalg.norm(drag_dv)\n",
    "        velocity_norm = np.linalg.norm(self.velocity)\n",
    "        if drag_dv_norm > 0.25*velocity_norm:\n",
    "            print(f'Warning: drag dv is {100*drag_dv_norm/velocity_norm}% of velocity.')\n",
    "        self.velocity = self.velocity + drag_dv\n",
    "    \n",
    "    def apply_gravity(self):\n",
    "        self.velocity = self.velocity + self.time_step*self.g\n",
    "    \n",
    "    def advance(self, report = True):\n",
    "        self.apply_gravity()\n",
    "        self.apply_drag()\n",
    "        self.position = self.position + self.velocity * self.time_step \n",
    "        self.history_position.append(self.position)\n",
    "        self.history_velocity.append(self.velocity)\n",
    "        self.stopwatch += self.time_step\n",
    "        if report:\n",
    "            state = np.array([self.position, self.velocity, [self.dry_mass, self.wet_mass]])\n",
    "            state = np.reshape(state, (1,6,))                \n",
    "            return state\n",
    "        \n",
    "    def is_crashed(self, ground_level):\n",
    "        if self.position[1] < ground_level:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def plot_trajectory(self, label = None, to_show = True, figsize = (12,12)):\n",
    "        x = np.array(self.history_position)[:,0]\n",
    "        y = np.array(self.history_position)[:,1]\n",
    "        if to_show:\n",
    "            plt.figure(figsize = figsize)\n",
    "            plt.plot(x,y, label = label)\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.plot(x,y, label = label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "4e4b264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs are the state of the rocket. Six values returned from advance()\n",
    "num_inputs = 6\n",
    "#actions are fire right, up, left, down, none\n",
    "num_actions = 5\n",
    "num_hidden = 32\n",
    "\n",
    "inputs = keras.layers.Input(shape = (num_inputs,))\n",
    "common1 = keras.layers.Dense(num_hidden, activation = 'relu')(inputs)\n",
    "# action = keras.layers.Dense(num_actions, activation = 'softmax')(common1)\n",
    "# critic = keras.layers.Dense(1)(common1)\n",
    "common2 = keras.layers.Dense(num_hidden, activation = 'relu')(common1)\n",
    "action = keras.layers.Dense(num_actions, activation = 'softmax')(common2)\n",
    "critic = keras.layers.Dense(1)(common2)\n",
    "\n",
    "model = keras.Model(inputs = inputs, outputs = [action, critic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "4c664853",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vel = np.array([250., 0.])\n",
    "target_alt = 250.\n",
    "\n",
    "#gives error values for the velocity and altitude\n",
    "def calculate_errors(rocket_state, target_velocity = target_vel, target_altitude = target_alt):\n",
    "    pos_x, altitude, vel_x, vel_y, dry_mass, wet_mass = rocket_state[0]\n",
    "    velocity = np.array([vel_x, vel_y])\n",
    "    speed_square_error = np.linalg.norm(velocity - target_velocity)**2\n",
    "    altitude_square_error = (altitude - target_altitude)**2\n",
    "    return speed_square_error, altitude_square_error\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "40ce67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2022\n",
    "gamma = 0.99\n",
    "max_simulation_time = 60*2 #seconds\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "e891f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "bf438ed2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15376/3348319293.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mactor_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mabs_diff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             critic_losses.append(\n\u001b[1;32m---> 62\u001b[1;33m                 \u001b[0mhuber\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             )\n\u001b[0;32m     64\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_losses\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mcall_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m       \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m       return losses_utils.compute_weighted_loss(\n\u001b[0m\u001b[0;32m    143\u001b[0m           losses, sample_weight, reduction=self._get_reduction())\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\losses_utils.py\u001b[0m in \u001b[0;36mcompute_weighted_loss\u001b[1;34m(losses, sample_weight, reduction, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;31m# Apply reduction function to the individual weighted losses.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce_weighted_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m     \u001b[1;31m# Convert the result back to the input type.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\losses_utils.py\u001b[0m in \u001b[0;36mreduce_weighted_loss\u001b[1;34m(weighted_losses, reduction)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mReductionV2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSUM_OVER_BATCH_SIZE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_num_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\losses_utils.py\u001b[0m in \u001b[0;36m_safe_mean\u001b[1;34m(losses, num_present)\u001b[0m\n\u001b[0;32m    248\u001b[0m       \u001b[0mthen\u001b[0m \u001b[0mzero\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m   \"\"\"\n\u001b[1;32m--> 250\u001b[1;33m   \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdivide_no_nan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_present\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[1;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[0;32m   2309\u001b[0m   \"\"\"\n\u001b[0;32m   2310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2311\u001b[1;33m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0m\u001b[0;32m   2312\u001b[0m                               _ReductionDims(input_tensor, axis))\n\u001b[0;32m   2313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mreduce_sum_with_dims\u001b[1;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[0;32m   2321\u001b[0m   return _may_reduce_to_scalar(\n\u001b[0;32m   2322\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2323\u001b[1;33m       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\n\u001b[0m\u001b[0;32m   2324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[1;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[0;32m  11189\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11190\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 11191\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m  11192\u001b[0m         _ctx, \"Sum\", name, input, axis, \"keep_dims\", keep_dims)\n\u001b[0;32m  11193\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate = 0.1)\n",
    "huber = tf.losses.Huber()\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "errors_history = []\n",
    "episode_count = 0\n",
    "\n",
    "plt.figure(figsize = (12,12))\n",
    "\n",
    "num_episodes = 1000\n",
    "episode_update_period = 50\n",
    "\n",
    "for epoch in range(num_episodes):\n",
    "    rkt = cartesian_rocket()\n",
    "    starting_state = np.array([rkt.position, rkt.velocity, [rkt.dry_mass, rkt.wet_mass]])\n",
    "    starting_state = np.reshape(starting_state, (1,6))\n",
    "    state = tf.convert_to_tensor(starting_state)\n",
    "    \n",
    "    episode_error = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        while not rkt.is_crashed(ground_level = -25.) and rkt.stopwatch <= max_simulation_time:\n",
    "                \n",
    "            action_probs, critic_value = model(state)\n",
    "            critic_value_history.append(critic_value[0,0])\n",
    "\n",
    "            action = rng.choice(num_actions, p = np.squeeze(action_probs))\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "\n",
    "            #fire in the appropriate direction unless the action is number 5: coast\n",
    "            if action == 0:\n",
    "                pass\n",
    "            else:\n",
    "                rkt.fire(action)\n",
    "\n",
    "            state = rkt.advance()\n",
    "            \n",
    "            velocity_error, altitude_error = calculate_errors(state)\n",
    "            total_error = velocity_error+altitude_error\n",
    "            errors_history.append(total_error)\n",
    "            episode_error += total_error\n",
    "        \n",
    "#         running_reward = 0.05 * episode_reward + (1-0.05) * running_reward\n",
    "        \n",
    "        errors = []\n",
    "        discounted_sum = 0\n",
    "        for r in errors_history[::-1]:\n",
    "            discounted_sum = r + gamma*discounted_sum\n",
    "            errors.insert(0, discounted_sum)\n",
    "        errors = np.array(errors)\n",
    "#         errors = (errors - np.mean(errors)) / (np.std(errors) + eps)\n",
    "        errors = errors/len(errors)\n",
    "        errors = errors.tolist()\n",
    "        \n",
    "        history = zip(action_probs_history, critic_value_history, errors)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, critic_error, true_error in history:\n",
    "            abs_diff = abs(critic_error - true_error)\n",
    "            actor_losses.append(-log_prob*abs_diff)\n",
    "            critic_losses.append(\n",
    "                huber(tf.expand_dims(critic_error, 0), tf.expand_dims(true_error,0))\n",
    "            )\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "               \n",
    "        \n",
    "    episode_count += 1\n",
    "    if episode_count % episode_update_period == 0:\n",
    "        template = 'average loss: {:.3f}/sec at episode {}, rocket lasted {:.2f} sec'\n",
    "        print(template.format(loss_value/rkt.stopwatch, episode_count, rkt.stopwatch))\n",
    "#         model.save(f'./actor_critic_rocket_models/model_{episode_count}.tf')\n",
    "        rkt.plot_trajectory(label='episode {}, remaining fuel {:.2f}, stopwatch {:.2f}'.format(episode_count,rkt.wet_mass,rkt.stopwatch), to_show=False)\n",
    "\n",
    "#         print(action_probs_history)\n",
    "        \n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "    \n",
    "# plt.xlim(-10, 500)\n",
    "# plt.ylim(-10, 500)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9c691f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b153b7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.06874639>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6b5ea446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.008754834>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.008995296>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00923361>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.009507009>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.009770889>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.009977623>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.01012019>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.010235712>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.010319095>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.010409157>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.010413328>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0103259375>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.010230613>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.010035269>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.009737034>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.009334897>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.008830013>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.008226096>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.007529776>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.006751054>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0059037437>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0050059916>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0040808115>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0031566801>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0027513865>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0028430629>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0029304994>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0030110907>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.003082102>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.003140694>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.003183971>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0032090268>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0032130163>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.003193231>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.003147187>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0030727298>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.002968156>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0028323533>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0026649493>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.002466487>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0022386266>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0019843522>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0017082238>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0014166372>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0011181254>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00082368165>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00054712093>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00014741771>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00016182181>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00017541314>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00018742973>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00019705445>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0002034524>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0002058158>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00020342323>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00019570753>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00018234002>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00016332541>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00013911123>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00011071609>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=7.987086e-05>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=4.9180155e-05>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.2303713e-05>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=4.1572684e-06>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.1376351e-06>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.1371383e-05>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=7.499008e-05>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00017443458>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00033478873>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00057414715>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0005619592>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0014671918>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0014551367>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0014492924>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.001451277>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0014627955>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0014856906>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0015220059>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0015740641>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0016445537>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0017366329>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0018540462>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0020012585>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0021836055>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0024074612>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0026804293>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0030115526>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.003411544>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.003893048>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0044709225>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.005162555>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0059882053>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.006971383>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.008000473>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.007966657>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.007905771>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.007821615>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.007718091>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0075991433>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.007468729>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.007330782>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.007189188>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.007047775>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0069102994>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.006780451>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.006661869>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.006558164>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0064729494>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0064098993>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0063728043>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.006365654>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0063927313>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0064587253>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0065688654>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0067290766>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0069461577>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.006932986>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.006524802>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0064595696>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0063733477>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0062697376>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0061523756>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0060248915>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0058908756>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.005753853>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0056172684>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00548448>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0053587565>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0052432925>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.005141229>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.005055689>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.004989827>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0049468875>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.004930286>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.004943698>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0049911765>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0050772796>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0052072257>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0053870715>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.004149294>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0041068476>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0040450147>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0039667278>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0038749771>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0037727635>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0036630572>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0035487635>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.003432696>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.003317558>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0032059338>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0031002883>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0030029768>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.002916264>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0028423623>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0027834692>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0027418348>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0027198333>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0027200535>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.002745413>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0027992819>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0028856385>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0030092401>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0029813205>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0014199602>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0013762595>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0013233856>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0012633442>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0011981203>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0011296327>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0010596995>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0009900077>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0009220935>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00085733016>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0007969252>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0007419268>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0006932442>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0006516749>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00061795034>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0005927906>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0005769776>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00057144335>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00057737605>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0005963472>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00063045794>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00068251055>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.5669351e-05>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.15878665e-05>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=7.255168e-06>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.409546e-06>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=7.796102e-07>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.124361e-08>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.7212467e-06>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=6.258107e-06>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.3870183e-05>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.4581997e-05>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.8199145e-05>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.430212e-05>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=7.225049e-05>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=9.1197704e-05>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.000110117544>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00012784357>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0001431214>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.000154678>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00016130577>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00016196432>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00015590359>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00014280618>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00012295305>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00013509339>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00015214513>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00017435207>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00020197725>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.00023524267>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0009440787>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.001025701>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0011144509>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0012092937>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0013089362>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0014118078>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.001516052>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0016195255>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0017198117>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0018142412>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0018999291>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0019738213>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0020327645>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0020735785>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0020931594>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0020885975>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0020573135>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0019972196>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0020630802>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.002146581>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0022468609>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.002363033>]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "39cebdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_value_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "37a2bde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.08970995]], dtype=float32)>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6f3687f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25912548184410605,\n",
       " 0.26093037790925744,\n",
       " 0.2626955094760913,\n",
       " 0.26469270844450465,\n",
       " 0.2665932722630208,\n",
       " 0.26806439519185976,\n",
       " 0.2690700501186602,\n",
       " 0.2698797577494254,\n",
       " 0.27046135359018436,\n",
       " 0.27108691490655706,\n",
       " 0.27111581211393176,\n",
       " 0.27050897550934455,\n",
       " 0.2698441192964033,\n",
       " 0.2684718946533746,\n",
       " 0.2663508921351862,\n",
       " 0.2634388085186316,\n",
       " 0.2596924045500478,\n",
       " 0.25506746226534516,\n",
       " 0.24951874187803894,\n",
       " 0.24299993823092284,\n",
       " 0.23546363680696197,\n",
       " 0.22686126929493866,\n",
       " 0.2171430687053458,\n",
       " 0.20625802403197713,\n",
       " 0.20752890537175386,\n",
       " 0.20875462812507795,\n",
       " 0.20990537929474587,\n",
       " 0.21095094533675074,\n",
       " 0.21186067497921485,\n",
       " 0.21260344166575426,\n",
       " 0.21314760561948803,\n",
       " 0.21346097552385102,\n",
       " 0.21351076981634864,\n",
       " 0.21326357759133435,\n",
       " 0.21268531910787228,\n",
       " 0.21174120589868126,\n",
       " 0.2103957004761456,\n",
       " 0.20861247563131619,\n",
       " 0.2063543733217804,\n",
       " 0.2035833631442736,\n",
       " 0.2002605003878066,\n",
       " 0.19634588366310032,\n",
       " 0.1917986121040265,\n",
       " 0.1865767421367414,\n",
       " 0.18063724381214682,\n",
       " 0.17393595669725997,\n",
       " 0.1664275453210507,\n",
       " 0.1672960985930397,\n",
       " 0.16811542935869933,\n",
       " 0.16885568357113145,\n",
       " 0.169486606221989,\n",
       " 0.1699775041562236,\n",
       " 0.1702972085112181,\n",
       " 0.17041403677652206,\n",
       " 0.1702957544703501,\n",
       " 0.1699095364289739,\n",
       " 0.1692219277050975,\n",
       " 0.16819880407126905,\n",
       " 0.16680533212433418,\n",
       " 0.1650059289869058,\n",
       " 0.16276422160178017,\n",
       " 0.16004300561517815,\n",
       " 0.15680420384467456,\n",
       " 0.15300882432760546,\n",
       " 0.14861691794572573,\n",
       " 0.14358753562182933,\n",
       " 0.13787868508401646,\n",
       " 0.13144728719323184,\n",
       " 0.12424913182966855,\n",
       " 0.11623883333358413,\n",
       " 0.11660042991883102,\n",
       " 0.11690768322313572,\n",
       " 0.11713068747460224,\n",
       " 0.11723913541741342,\n",
       " 0.11720228112129817,\n",
       " 0.11698890241533401,\n",
       " 0.11656726294229629,\n",
       " 0.11590507382971843,\n",
       " 0.11496945497379096,\n",
       " 0.11372689593218537,\n",
       " 0.11214321642186295,\n",
       " 0.11018352641786344,\n",
       " 0.1078121858490565,\n",
       " 0.10499276388678103,\n",
       " 0.10168799782225055,\n",
       " 0.09785975152858616,\n",
       " 0.09346897350327386,\n",
       " 0.08847565448680497,\n",
       " 0.08283878465322254,\n",
       " 0.07651631036825203,\n",
       " 0.06946509051064163,\n",
       " 0.06164085235230495,\n",
       " 0.0529981469928132,\n",
       " 0.05302253917789916,\n",
       " 0.05329014394383596,\n",
       " 0.05377342722005022,\n",
       " 0.054444471892301706,\n",
       " 0.055274940750530466,\n",
       " 0.056236039061938685,\n",
       " 0.057298476765537835,\n",
       " 0.058432430284302984,\n",
       " 0.05960750395108142,\n",
       " 0.060792691044351865,\n",
       " 0.06195633442986819,\n",
       " 0.06306608680422472,\n",
       " 0.0640888705362913,\n",
       " 0.06499083710246942,\n",
       " 0.06573732611164779,\n",
       " 0.06629282391571042,\n",
       " 0.0666209218014001,\n",
       " 0.0666842737593101,\n",
       " 0.06644455382571729,\n",
       " 0.0658624129929327,\n",
       " 0.06489743568381437,\n",
       " 0.06350809578602512,\n",
       " 0.061651712241577575,\n",
       " 0.06176351417665115,\n",
       " 0.062119411619343194,\n",
       " 0.06269187941753152,\n",
       " 0.0634530094655152,\n",
       " 0.0643744736527656,\n",
       " 0.0654274864379339,\n",
       " 0.06658276704432126,\n",
       " 0.06781050127297364,\n",
       " 0.06908030292953761,\n",
       " 0.07036117486097511,\n",
       " 0.07162146959817434,\n",
       " 0.07282884960049388,\n",
       " 0.07395024709817871,\n",
       " 0.07495182352861847,\n",
       " 0.07579892856230303,\n",
       " 0.07645605871435159,\n",
       " 0.07688681553740176,\n",
       " 0.07705386339163502,\n",
       " 0.0769188867876616,\n",
       " 0.07644254729792728,\n",
       " 0.07558444003229381,\n",
       " 0.07430304967337718,\n",
       " 0.07255570606718573,\n",
       " 0.07277764935403346,\n",
       " 0.07324480068740641,\n",
       " 0.0739296461529491,\n",
       " 0.07480428899624075,\n",
       " 0.07584041257269118,\n",
       " 0.07700924292270724,\n",
       " 0.07828151096833413,\n",
       " 0.07962741432753143,\n",
       " 0.08101657874222133,\n",
       " 0.08241801911621087,\n",
       " 0.08380010015901859,\n",
       " 0.0851304966316495,\n",
       " 0.08637615319025534,\n",
       " 0.0875032438236448,\n",
       " 0.08847713088051208,\n",
       " 0.08926232368224007,\n",
       " 0.08982243671708673,\n",
       " 0.09012014741151846,\n",
       " 0.09011715347441253,\n",
       " 0.08977412980979602,\n",
       " 0.08905068499377658,\n",
       " 0.08790531731123943,\n",
       " 0.0862953703478547,\n",
       " 0.0866560981223865,\n",
       " 0.08726343580695413,\n",
       " 0.08808988364744161,\n",
       " 0.08910755919269725,\n",
       " 0.09028816024587932,\n",
       " 0.09160292744107953,\n",
       " 0.09302260644143848,\n",
       " 0.09451740975490945,\n",
       " 0.09605697816381568,\n",
       " 0.09761034176428571,\n",
       " 0.0991458806116199,\n",
       " 0.10063128496761009,\n",
       " 0.10203351514577051,\n",
       " 0.10331876095042766,\n",
       " 0.10445240070554472,\n",
       " 0.10539895986914168,\n",
       " 0.10612206922910906,\n",
       " 0.10658442267618806,\n",
       " 0.10674773454983631,\n",
       " 0.10657269655264832,\n",
       " 0.10601893422898148,\n",
       " 0.10504496300336516,\n",
       " 0.10360814377424464,\n",
       " 0.10414374804803189,\n",
       " 0.10492772866114136,\n",
       " 0.1059326037021763,\n",
       " 0.10713050874293463,\n",
       " 0.10849315979157353,\n",
       " 0.10999181587107332,\n",
       " 0.11159724121920984,\n",
       " 0.11327966710619355,\n",
       " 0.11500875326612292,\n",
       " 0.11675354893833413,\n",
       " 0.11848245351469879,\n",
       " 0.1201631767889023,\n",
       " 0.12176269880364192,\n",
       " 0.12324722929171245,\n",
       " 0.12458216670684222,\n",
       " 0.1257320568401492,\n",
       " 0.12666055101800555,\n",
       " 0.12733036387709382,\n",
       " 0.12770323071236694,\n",
       " 0.12773986439358834,\n",
       " 0.12739991184609203,\n",
       " 0.1266419100913561,\n",
       " 0.12542324184292192,\n",
       " 0.12617920064265567,\n",
       " 0.1271857615850043,\n",
       " 0.1284154652414324,\n",
       " 0.1298404698936981,\n",
       " 0.13143251448931487,\n",
       " 0.13316288122232756,\n",
       " 0.1350023577356278,\n",
       " 0.13692119894095997,\n",
       " 0.13888908845275574,\n",
       " 0.14087509963190176,\n",
       " 0.1428476562354742,\n",
       " 0.14477449266847342,\n",
       " 0.14662261383351177,\n",
       " 0.1483582545744094,\n",
       " 0.14994683870956674,\n",
       " 0.1513529376509814,\n",
       " 0.15254022860470423,\n",
       " 0.1534714523485062,\n",
       " 0.15410837058248053,\n",
       " 0.15441172284824842,\n",
       " 0.1543411830124156,\n",
       " 0.15385531530986402,\n",
       " 0.1529115299424247,\n",
       " 0.15394514821791072,\n",
       " 0.1552321732771813,\n",
       " 0.1567451740214086,\n",
       " 0.15845633734822012,\n",
       " 0.1603374311100441,\n",
       " 0.1623597666978115,\n",
       " 0.1644941612462173,\n",
       " 0.16671089945670645,\n",
       " 0.16897969503431795,\n",
       " 0.1712696517344904,\n",
       " 0.17354922401586662,\n",
       " 0.17578617729513177,\n",
       " 0.1779475477998336,\n",
       " 0.1799996020151379,\n",
       " 0.18190779572040414,\n",
       " 0.18363673261142355,\n",
       " 0.18515012250414076,\n",
       " 0.18641073911561368,\n",
       " 0.18738037741794278,\n",
       " 0.18801981056083744,\n",
       " 0.1882887463584652,\n",
       " 0.18814578333617726,\n",
       " 0.18754836633264055,\n",
       " 0.1889318516423722,\n",
       " 0.19057227774633384,\n",
       " 0.1924422492427752,\n",
       " 0.19451398908697407,\n",
       " 0.1967593015532296,\n",
       " 0.19914953482224135,\n",
       " 0.20165554319008586,\n",
       " 0.20424764889495717,\n",
       " 0.20689560355780343,\n",
       " 0.2095685492329599,\n",
       " 0.21223497906482658,\n",
       " 0.21486269754660695,\n",
       " 0.21741878037708132,\n",
       " 0.21986953391134775,\n",
       " 0.222180454201424,\n",
       " 0.22431618562255531,\n",
       " 0.22624047908104208,\n",
       " 0.22791614979935287,\n",
       " 0.2293050346742454,\n",
       " 0.23036794920356662,\n",
       " 0.23106464397738344,\n",
       " 0.23135376072902353,\n",
       " 0.2311927879415756,\n",
       " 0.23301712599483132,\n",
       " 0.23510285790033353,\n",
       " 0.23742263323671378,\n",
       " 0.23994872039398288,\n",
       " 0.24265296954010696,\n",
       " 0.24550677521302675,\n",
       " 0.24848103853431355,\n",
       " 0.25154612904064205,\n",
       " 0.2546718461292022,\n",
       " 0.2575316466274848,\n",
       " 0.2603874981178768,\n",
       " 0.2632072369348063,\n",
       " 0.26536789776138925,\n",
       " 0.2674226098053408,\n",
       " 0.2689896762712162,\n",
       " 0.2703446041961547,\n",
       " 0.27145034592572637,\n",
       " 0.27203019010358404,\n",
       " 0.27231788057297707,\n",
       " 0.27203735153250236,\n",
       " 0.27118263034707246,\n",
       " 0.2696893705232394,\n",
       " 0.2678107357443796,\n",
       " 0.2700049520582693,\n",
       " 0.2718610955702936,\n",
       " 0.2739515266837589,\n",
       " 0.27624852179519865,\n",
       " 0.27872393918397814,\n",
       " 0.28134918160552846,\n",
       " 0.28409515850623124,\n",
       " 0.28693224785613125,\n",
       " 0.2895727420035535,\n",
       " 0.29201670088278114,\n",
       " 0.2942642781461964,\n",
       " 0.29657500227684086,\n",
       " 0.29831595602978134,\n",
       " 0.2997963176558331,\n",
       " 0.30097092605454895,\n",
       " 0.3018049912465535,\n",
       " 0.30226298711463817,\n",
       " 0.30261158081748135,\n",
       " 0.3028160263636614,\n",
       " 0.3022361862748358,\n",
       " 0.30082813652209733,\n",
       " 0.2991596909589715,\n",
       " 0.2971924307536921,\n",
       " 0.294582645379481,\n",
       " 0.2915917684578022,\n",
       " 0.2940261972233436,\n",
       " 0.29672818237357806,\n",
       " 0.299068495241412,\n",
       " 0.30161790170549996,\n",
       " 0.3037437430068855,\n",
       " 0.30575224357718084,\n",
       " 0.30760707622522404,\n",
       " 0.30958092854188646,\n",
       " 0.3112991791147705,\n",
       " 0.3124319476328832,\n",
       " 0.31329205840423524,\n",
       " 0.31414109273457796,\n",
       " 0.31464053970887984,\n",
       " 0.31444739799921556,\n",
       " 0.3141381944397722,\n",
       " 0.3136783141308895,\n",
       " 0.3130322925591,\n",
       " 0.31154992480474347,\n",
       " 0.3098042950785325,\n",
       " 0.307140529407783,\n",
       " 0.3035117081915185,\n",
       " 0.2994946234509226,\n",
       " 0.29504773256739314,\n",
       " 0.2969126158608428,\n",
       " 0.29843354751994583,\n",
       " 0.30018793854295767,\n",
       " 0.3021480624886326,\n",
       " 0.30428577479384433,\n",
       " 0.30596407948007753,\n",
       " 0.3074132078230137,\n",
       " 0.30891449128024195,\n",
       " 0.3104371171678371,\n",
       " 0.3116890240200767,\n",
       " 0.3126248259218931,\n",
       " 0.31325558539307125,\n",
       " 0.3134867938777412,\n",
       " 0.3136029076807979,\n",
       " 0.3135693373968385,\n",
       " 0.3127382019320017,\n",
       " 0.31142782515316303,\n",
       " 0.3092708998491136,\n",
       " 0.3068443309950371,\n",
       " 0.3041096383530415,\n",
       " 0.3007183807925089,\n",
       " 0.29693650847703296,\n",
       " 0.2927223301388672,\n",
       " 0.29489736576080056,\n",
       " 0.2970678245132725,\n",
       " 0.2995066116823818,\n",
       " 0.30218645609981565,\n",
       " 0.3047787384059523,\n",
       " 0.3069507364769852,\n",
       " 0.3092736380510184,\n",
       " 0.31141611683219467,\n",
       " 0.3133860031796702,\n",
       " 0.315183555984823,\n",
       " 0.31707276702232384,\n",
       " 0.3187204300899206,\n",
       " 0.32009326973604313,\n",
       " 0.32146028421666645,\n",
       " 0.32278924037963935,\n",
       " 0.32374557556926475,\n",
       " 0.32434149081265296,\n",
       " 0.32482838617061965,\n",
       " 0.32481871182255617,\n",
       " 0.32459323642554155,\n",
       " 0.32381319225045924,\n",
       " 0.3227396413827166,\n",
       " 0.3207302547246726,\n",
       " 0.31773735204603964,\n",
       " 0.3140184055263217,\n",
       " 0.31667936597942337,\n",
       " 0.31930920904923576,\n",
       " 0.32191154227767727,\n",
       " 0.3247565859484438,\n",
       " 0.3275503026767801,\n",
       " 0.3299569135523634,\n",
       " 0.3319409275172569,\n",
       " 0.3338104487490984,\n",
       " 0.3355293290732277,\n",
       " 0.33706602984821143,\n",
       " 0.33808095450975273,\n",
       " 0.33853634598317595,\n",
       " 0.3390154493064491,\n",
       " 0.3391772122126401,\n",
       " 0.3392964945146488,\n",
       " 0.3390314771357727,\n",
       " 0.3386548778740661,\n",
       " 0.33782395469455956,\n",
       " 0.33680911405535785,\n",
       " 0.335266531578207,\n",
       " 0.3334641296842271,\n",
       " 0.3307486016428362,\n",
       " 0.3276923695089333,\n",
       " 0.324005906119395,\n",
       " 0.3199318608225747,\n",
       " 0.32265255314735575,\n",
       " 0.3253427314410867,\n",
       " 0.3279727306313681,\n",
       " 0.33054700133300235,\n",
       " 0.33303257185471313,\n",
       " 0.3350960394160349,\n",
       " 0.33730980584293385,\n",
       " 0.33903766351501313,\n",
       " 0.3405487088782511,\n",
       " 0.34150412135107333,\n",
       " 0.34212980545078564,\n",
       " 0.3424024411490487,\n",
       " 0.3422867000174475,\n",
       " 0.3417945314985608,\n",
       " 0.3409230291674814,\n",
       " 0.3396693665398726,\n",
       " 0.33829181324987345,\n",
       " 0.3364466075821195,\n",
       " 0.33409554595365254,\n",
       " 0.3315083827117047,\n",
       " 0.3283393121451154,\n",
       " 0.32449013745985605,\n",
       " 0.3199399898068104,\n",
       " 0.3146448403156838,\n",
       " 0.31704131543435277,\n",
       " 0.31940340587089916,\n",
       " 0.3213977980339509,\n",
       " 0.3236003350834244,\n",
       " 0.32537322261788243,\n",
       " 0.3272940207997282,\n",
       " 0.3290668230240978,\n",
       " 0.33065358181925764,\n",
       " 0.33202219415276,\n",
       " 0.3328324802571046,\n",
       " 0.33339827621176704,\n",
       " 0.33336599682720014,\n",
       " 0.3330528623696638,\n",
       " 0.33245790958237176,\n",
       " 0.3315802674231128,\n",
       " 0.33006035725837257,\n",
       " 0.32817414642176673,\n",
       " 0.32588576753531595,\n",
       " 0.3231585759750299,\n",
       " 0.32027004357132155,\n",
       " 0.3171843697642122,\n",
       " 0.31349662639220927,\n",
       " 0.3088697820766372,\n",
       " 0.3032566344755023,\n",
       " 0.2969881313545476,\n",
       " 0.2896770464548846,\n",
       " 0.29182132163556507,\n",
       " 0.2942629473202615,\n",
       " 0.2967054342458379,\n",
       " 0.2991190365341528,\n",
       " 0.30177636507932504,\n",
       " 0.3043827310007622,\n",
       " 0.3066018144869418,\n",
       " 0.30839813940434146,\n",
       " 0.31004321755929887,\n",
       " 0.3114657181173909,\n",
       " 0.31233242975521747,\n",
       " 0.3129152624583517,\n",
       " 0.3131803572830645,\n",
       " 0.3134020159623727,\n",
       " 0.3129296517516787,\n",
       " 0.3119839610395774,\n",
       " 0.31085434478062174,\n",
       " 0.30919450991163633,\n",
       " 0.306965006989872,\n",
       " 0.3041253931618605,\n",
       " 0.3009444330642337,\n",
       " 0.2967615243326139,\n",
       " 0.29190111005784275,\n",
       " 0.2862944976621401,\n",
       " 0.2802088875972046,\n",
       " 0.2819238835677231,\n",
       " 0.28390172116132156,\n",
       " 0.2858455333766899,\n",
       " 0.2880274018479245,\n",
       " 0.2904196239062778,\n",
       " 0.2929940815376251,\n",
       " 0.29542059168668844,\n",
       " 0.2973647809192302,\n",
       " 0.2991345172626944,\n",
       " 0.3009943509773184,\n",
       " 0.30261044261757253,\n",
       " 0.30399232721618985,\n",
       " 0.3054009902916287,\n",
       " 0.3065012521330927,\n",
       " 0.3072584400431205,\n",
       " 0.30794072552247487,\n",
       " 0.3082114001999845,\n",
       " 0.30833639510866173,\n",
       " 0.30797801446993833,\n",
       " 0.3071504165188302,\n",
       " 0.3058494199138439,\n",
       " 0.3039631053271228,\n",
       " 0.3015263478268879,\n",
       " 0.2981710753802494,\n",
       " 0.2944654285700126,\n",
       " 0.29665807124680577,\n",
       " 0.2988463149031774,\n",
       " 0.3013030666216793,\n",
       " 0.3037327078822397,\n",
       " 0.30610394606838864,\n",
       " 0.30838673021085306,\n",
       " 0.31085283571512085,\n",
       " 0.3131337101593587,\n",
       " 0.3152435830697361,\n",
       " 0.31714288744352537,\n",
       " 0.31849482062201595,\n",
       " 0.3192611447160789,\n",
       " 0.3200179688994377,\n",
       " 0.32042690076394625,\n",
       " 0.32014506385348757,\n",
       " 0.3194890700833999,\n",
       " 0.3187167859590226,\n",
       " 0.3174340559899199,\n",
       " 0.3159283592910068,\n",
       " 0.3139086426976386,\n",
       " 0.31162432834674764,\n",
       " 0.30903708464870905,\n",
       " 0.3058577278294541,\n",
       " 0.30171249438086106,\n",
       " 0.2968038864933407,\n",
       " 0.29868650871533586,\n",
       " 0.30083366575486986,\n",
       " 0.3029485077136081,\n",
       " 0.30469476544565227,\n",
       " 0.3063787364296954,\n",
       " 0.3079674485929246,\n",
       " 0.30973646636924923,\n",
       " 0.31138895271297806,\n",
       " 0.3131949201305627,\n",
       " 0.3148197279970164,\n",
       " 0.31653683368306407,\n",
       " 0.31805252040417137,\n",
       " 0.31932547850442433,\n",
       " 0.32032240541758117,\n",
       " 0.32070247129962265,\n",
       " 0.3207803546933396,\n",
       " 0.3205091121100542,\n",
       " 0.31954365569957727,\n",
       " 0.31820264590340136,\n",
       " 0.3163851007698062,\n",
       " 0.3137557949714215,\n",
       " 0.3102706181905383,\n",
       " 0.30588445364851236,\n",
       " 0.3011881475788235,\n",
       " 0.2958251536926175,\n",
       " 0.2900696402387756,\n",
       " 0.28362453975479507,\n",
       " 0.28597849146272974,\n",
       " 0.28829822468893995,\n",
       " 0.29085499890917826,\n",
       " 0.29332079025478935,\n",
       " 0.2957009293009861,\n",
       " 0.2982617802454569,\n",
       " 0.300974738786961,\n",
       " 0.30321003715902395,\n",
       " 0.3052336244235292,\n",
       " 0.30705414549486254,\n",
       " 0.3086714746631344,\n",
       " 0.3100441686117492,\n",
       " 0.3108345778163225,\n",
       " 0.31130986709293124,\n",
       " 0.3114352699939887,\n",
       " 0.31122215764920347,\n",
       " 0.3109271017164489,\n",
       " 0.31025965984142523,\n",
       " 0.3092173923571948,\n",
       " 0.3080554838089173,\n",
       " 0.30643327870298515,\n",
       " 0.30436416208913175,\n",
       " 0.30209916342338705,\n",
       " 0.29898877006826075,\n",
       " 0.2949875784572693,\n",
       " 0.29036023471144906,\n",
       " 0.2927822237421779,\n",
       " 0.29486844070555485,\n",
       " 0.29692172703181163,\n",
       " 0.2989106363575857,\n",
       " 0.30050021326980664,\n",
       " 0.30226715146454236,\n",
       " 0.3035718274428569,\n",
       " 0.3043776405657632,\n",
       " 0.30464737969565814,\n",
       " 0.3049660790908084,\n",
       " 0.3046812269791656,\n",
       " 0.304111386989748,\n",
       " 0.3035239961111624,\n",
       " 0.30226098968732906,\n",
       " 0.30054850301743635,\n",
       " 0.29836440687655835,\n",
       " 0.2959856684535039,\n",
       " 0.2933759514308171,\n",
       " 0.2901843959952353,\n",
       " 0.2863702590142827,\n",
       " 0.28194938805389436,\n",
       " 0.2765417781940975,\n",
       " 0.2707331735578924,\n",
       " 0.2638490898148678,\n",
       " 0.26573247654463955,\n",
       " 0.26760834045654464,\n",
       " 0.2694140630344298,\n",
       " 0.2714241318824776,\n",
       " 0.2736104118510031,\n",
       " 0.27567993308762556,\n",
       " 0.27789867325905676,\n",
       " 0.27997453756908697,\n",
       " 0.28186960109581916,\n",
       " 0.28359177027725235,\n",
       " 0.2851414597309752,\n",
       " 0.28678268088287595,\n",
       " 0.2881821383433329,\n",
       " 0.28960941703448523,\n",
       " 0.2910329015680639,\n",
       " 0.29211870465774425,\n",
       " 0.29287853725719204,\n",
       " 0.29326352972041647,\n",
       " 0.2931874595403434,\n",
       " 0.2929312761181458,\n",
       " 0.2922094330077134,\n",
       " 0.29101796490066395,\n",
       " 0.2893529981317215,\n",
       " 0.28746149285532285,\n",
       " 0.285305707078879,\n",
       " 0.2822398424468033,\n",
       " 0.278216694034997,\n",
       " 0.2802452080801238,\n",
       " 0.2822676652398623,\n",
       " 0.2845569558506545,\n",
       " 0.28681744447716484,\n",
       " 0.2890510772400706,\n",
       " 0.29152955077285764,\n",
       " 0.2942252222313162,\n",
       " 0.2965054908987386,\n",
       " 0.2989429182879786,\n",
       " 0.3012055576590579,\n",
       " 0.3032614715851123,\n",
       " 0.30503558251007434,\n",
       " 0.3065041068836801,\n",
       " 0.3079362244160164,\n",
       " 0.30869198828268046,\n",
       " 0.30903668171104026,\n",
       " 0.3089335456496944,\n",
       " 0.3086498877623254,\n",
       " 0.30753972546213254,\n",
       " 0.3055585056515675,\n",
       " 0.30266062148407236,\n",
       " 0.29942141597235905,\n",
       " 0.2954910393389342,\n",
       " 0.29082576074322863,\n",
       " 0.2850696476520099,\n",
       " 0.2871673834508438,\n",
       " 0.2895619996589273,\n",
       " 0.29222657733915886,\n",
       " 0.29483288605615654,\n",
       " 0.2973508898770038,\n",
       " 0.2997501193779984,\n",
       " 0.3016973739278361,\n",
       " 0.30346041127043144,\n",
       " 0.30500720406744924,\n",
       " 0.3066080523521502,\n",
       " 0.30823213496385,\n",
       " 0.309590227992211,\n",
       " 0.31033363785512336,\n",
       " 0.31072824627958284,\n",
       " 0.3107854144702144,\n",
       " 0.31050301777020234,\n",
       " 0.30987901693919323,\n",
       " 0.30886430242760177,\n",
       " 0.3074223084929975,\n",
       " 0.3052080579157046,\n",
       " 0.302796062207496,\n",
       " 0.29984198851440047,\n",
       " 0.2963057469167534,\n",
       " 0.29245386538056994,\n",
       " 0.288246673802927,\n",
       " 0.28364344747218945,\n",
       " 0.2857267772086006,\n",
       " 0.28810684183847934,\n",
       " 0.2907567209548675,\n",
       " 0.2933481826376803,\n",
       " 0.29615215971245346,\n",
       " 0.2988402510861005,\n",
       " 0.3014187099501219,\n",
       " 0.3035491197511335,\n",
       " 0.30580219249386376,\n",
       " 0.3081483118814669,\n",
       " 0.3099527207147612,\n",
       " 0.31148213680519865,\n",
       " 0.3130063078680008,\n",
       " 0.31449307576302415,\n",
       " 0.3156543028836042,\n",
       " 0.3163925452839243,\n",
       " 0.31663442027313976,\n",
       " 0.3163588791105682,\n",
       " 0.3155808857473495,\n",
       " 0.3142410097073229,\n",
       " 0.312298677222781,\n",
       " 0.30971228031736503,\n",
       " 0.30613281763716527,\n",
       " 0.3021266983716108,\n",
       " 0.30439672761205705,\n",
       " 0.3063596225650719,\n",
       " 0.30828651124568507,\n",
       " 0.3101476795590277,\n",
       " 0.3118772234988041,\n",
       " 0.31375305962561123,\n",
       " 0.3154804259108452,\n",
       " 0.3167151750078047,\n",
       " 0.3180351109520264,\n",
       " 0.3190611777552666,\n",
       " 0.31976960156641143,\n",
       " 0.32043265175564833,\n",
       " 0.3204037492634232,\n",
       " 0.31964170768625316,\n",
       " 0.3184159826890807,\n",
       " 0.3169989335907018,\n",
       " 0.31473385991447644,\n",
       " 0.311888829743055,\n",
       " 0.3087347073942352,\n",
       " 0.3052322321044956,\n",
       " 0.3013410775036608,\n",
       " 0.2967709018721711,\n",
       " 0.29151362075684084,\n",
       " 0.29394726015166517,\n",
       " 0.2966484479577411,\n",
       " 0.29925492155114275,\n",
       " 0.3020401599281509,\n",
       " 0.30497559393203305,\n",
       " 0.3077324730467684,\n",
       " 0.31057851085621346,\n",
       " 0.31322586030939975,\n",
       " 0.3156744070598785,\n",
       " 0.3178825998144453,\n",
       " 0.3198171373890797,\n",
       " 0.32139853105460076,\n",
       " 0.3226039564801947,\n",
       " 0.3233973224899567,\n",
       " 0.32374170969070265,\n",
       " 0.323900321414613,\n",
       " 0.3238368293678454,\n",
       " 0.32291333196301764,\n",
       " 0.3213300281131912,\n",
       " 0.31906675428140757,\n",
       " 0.31607987042100205,\n",
       " 0.3123245635515776,\n",
       " 0.31496841448978263,\n",
       " 0.3175809752213162,\n",
       " 0.3201325728336208,\n",
       " 0.3228940972491751,\n",
       " 0.325235505737926,\n",
       " 0.327120859779223,\n",
       " 0.328857691135348,\n",
       " 0.3304079632707079,\n",
       " 0.33204442373150983,\n",
       " 0.3333899779192132,\n",
       " 0.3347256949058199,\n",
       " 0.3360194148784425,\n",
       " 0.33698160694452384,\n",
       " 0.3376102215171813,\n",
       " 0.33785610288150647,\n",
       " 0.3379878566488505,\n",
       " 0.33766651785820784,\n",
       " 0.33680127569259116,\n",
       " 0.3353152427621425,\n",
       " 0.3328811101084449,\n",
       " 0.3297601751684518,\n",
       " 0.3259087355139125,\n",
       " 0.32134387756170074,\n",
       " 0.32377724227046395,\n",
       " 0.3261771906490275,\n",
       " 0.32821116451972804,\n",
       " 0.33018338944400677,\n",
       " 0.33209536405280016,\n",
       " 0.33360801765550846,\n",
       " 0.33530056942346326,\n",
       " 0.3365308227645508,\n",
       " 0.3372621437561971,\n",
       " 0.33807878235033745,\n",
       " 0.33833026491901325,\n",
       " 0.33829088174779376,\n",
       " 0.33823804656500644,\n",
       " 0.33813952307976547,\n",
       " 0.3379623511146174,\n",
       " 0.3373633103418079,\n",
       " 0.3359945696536076,\n",
       " 0.33443748997770356,\n",
       " 0.3320326831337277,\n",
       " 0.3287352278430397,\n",
       " 0.32481547323106036,\n",
       " 0.3205465721393619,\n",
       " 0.31588814875323584,\n",
       " 0.3107987226503216,\n",
       " 0.31315634809560233,\n",
       " 0.3155419939352168,\n",
       " 0.31823030523944973,\n",
       " 0.32092417425764835,\n",
       " 0.3236259842723142,\n",
       " 0.32600138423777963,\n",
       " 0.32832259322068275,\n",
       " 0.3305941338431772,\n",
       " 0.33278346945407283,\n",
       " 0.3348959556366721,\n",
       " 0.33689752867944006,\n",
       " 0.3384491305376904,\n",
       " 0.3398628670494808,\n",
       " 0.3407904988381125,\n",
       " 0.3411948953559132,\n",
       " 0.34135319855376917,\n",
       " 0.3409172907199508,\n",
       " 0.3404806352105284,\n",
       " 0.3400111873139009,\n",
       " 0.33947619453544603,\n",
       " 0.33857697912731877,\n",
       " 0.3372630216180724,\n",
       " 0.3358117335166671,\n",
       " 0.33387421648192467,\n",
       " 0.3314647971977783,\n",
       " 0.3282109654643503,\n",
       " 0.3243855588966129,\n",
       " 0.3196300454408118,\n",
       " 0.3142750572309154,\n",
       " 0.3083149016317456,\n",
       " 0.31031379673392634,\n",
       " 0.31197009385639335,\n",
       " 0.3138612176706822,\n",
       " 0.31595945554694077,\n",
       " 0.3179684808969818,\n",
       " 0.3198896455650712,\n",
       " 0.321993866479602,\n",
       " 0.32394766716745155,\n",
       " 0.3257195169560598,\n",
       " 0.32731730960672917,\n",
       " 0.3287014201508285,\n",
       " 0.32983913607209764,\n",
       " 0.3303902793615604,\n",
       " 0.3306246039108544,\n",
       " 0.3305528325968336,\n",
       " 0.3301734858725547,\n",
       " 0.3294851734817185,\n",
       " 0.3287503735109157,\n",
       " 0.32757829105209063,\n",
       " 0.3259963245864874,\n",
       " 0.3242616391259283,\n",
       " 0.3217179713672737,\n",
       " 0.3186873762908648,\n",
       " 0.31511243864658434,\n",
       " 0.31126527779074425,\n",
       " 0.3068520517649746,\n",
       " 0.3018675865763884,\n",
       " 0.3038013572840707,\n",
       " 0.3060001794606629,\n",
       " 0.30843674513488356,\n",
       " 0.31074514972731004,\n",
       " 0.3128986034826338,\n",
       " 0.31490420903101995,\n",
       " 0.3164190076346312,\n",
       " 0.3180182197668877,\n",
       " 0.31940812251154016,\n",
       " 0.3208526282525542,\n",
       " 0.32205882074829123,\n",
       " 0.32328882391036573,\n",
       " 0.32425051591480797,\n",
       " 0.32489860879713556,\n",
       " 0.325198253048542,\n",
       " 0.32516112108284145,\n",
       " 0.32468931110715404,\n",
       " 0.323452067074668,\n",
       " 0.32140619345619137,\n",
       " 0.31881989195167426,\n",
       " 0.3157079788629225,\n",
       " 0.31201066166155167,\n",
       " 0.30799849895473014,\n",
       " 0.3033207810008345,\n",
       " 0.29824531076684296,\n",
       " 0.30047613407188684,\n",
       " 0.302399427069951,\n",
       " 0.3042863137758653,\n",
       " 0.3061403338115898,\n",
       " 0.30793013641709643,\n",
       " 0.30931949574108797,\n",
       " 0.3108876723313924,\n",
       " 0.31199250025712005,\n",
       " 0.3129068014519392,\n",
       " 0.3135984815492511,\n",
       " 0.31434297455159943,\n",
       " 0.31480197825395273,\n",
       " 0.3146327737606042,\n",
       " 0.31441585152375545,\n",
       " 0.31376134505930386,\n",
       " 0.31233642865673583,\n",
       " 0.31009823207092263,\n",
       " 0.307631305972445,\n",
       " 0.3046398793702147,\n",
       " 0.30137791593575064,\n",
       " 0.29780712371946344,\n",
       " 0.2935759603586175,\n",
       " 0.28870065225229985,\n",
       " 0.28311466535979185,\n",
       " 0.27708671591677597,\n",
       " 0.27910381604151624,\n",
       " 0.281416982073748,\n",
       " 0.28399928684907827,\n",
       " 0.28655581985698597,\n",
       " 0.2893569207491139,\n",
       " 0.2920739783134901,\n",
       " 0.29497753115541925,\n",
       " 0.2977387385003921,\n",
       " 0.30036474832153387,\n",
       " 0.30285640502168554,\n",
       " 0.3054776656781025,\n",
       " 0.3078979397116741,\n",
       " 0.31012599198222596,\n",
       " 0.3118177229924769,\n",
       " 0.31354343343461527,\n",
       " 0.3146643560496774,\n",
       " 0.31544714166559945,\n",
       " 0.31555013853064595,\n",
       " 0.3152404581325515,\n",
       " 0.314481144882525,\n",
       " 0.3129260413602308,\n",
       " 0.3108416689381241,\n",
       " 0.30818834631068515,\n",
       " 0.304615021154308,\n",
       " 0.3003872555865493,\n",
       " 0.2954622577034984,\n",
       " 0.28948367940205266,\n",
       " 0.29189681433874737,\n",
       " 0.29427632860410835,\n",
       " 0.2965579553836374,\n",
       " 0.29901576788262507,\n",
       " 0.30132020376024027,\n",
       " 0.3034397801087056,\n",
       " 0.30564343456182735,\n",
       " 0.30729898131825906,\n",
       " 0.30867177382900873,\n",
       " 0.3097718199438874,\n",
       " 0.3105537751624716,\n",
       " 0.31128583734902787,\n",
       " 0.3119349295987823,\n",
       " 0.3124672047383693,\n",
       " 0.312848004360107,\n",
       " 0.31279285188019246,\n",
       " 0.3125473540305517,\n",
       " 0.3120749403859639,\n",
       " 0.3110385070393141,\n",
       " 0.30969683154786815,\n",
       " 0.30776911683627306,\n",
       " 0.3048901767922867,\n",
       " 0.3016163656142116,\n",
       " 0.3035475987364174,\n",
       " 0.3054403434921955,\n",
       " 0.30729813296908437,\n",
       " 0.309054924810168,\n",
       " 0.3109878788527521,\n",
       " 0.3130683977214911,\n",
       " 0.31500245157630075,\n",
       " 0.31705623002417177,\n",
       " 0.3189365523866742,\n",
       " 0.3209079327641215,\n",
       " 0.32294009025821196,\n",
       " 0.32469923542816864,\n",
       " 0.32645435265179384,\n",
       " 0.32817321210230654,\n",
       " 0.3295212982266843,\n",
       " 0.3307640902574803,\n",
       " 0.3315661660828467,\n",
       " 0.33188978659959956,\n",
       " 0.3317492327934618,\n",
       " 0.33114033771267193,\n",
       " 0.329702167327618,\n",
       " 0.3279995346804771,\n",
       " 0.32569030411240796,\n",
       " 0.3227325151615399,\n",
       " 0.31914251234583996,\n",
       " 0.3215536410423218,\n",
       " ...]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d03dc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
